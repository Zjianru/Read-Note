# 线性回归

## 一、线性回归模型的基本形式

### 1.1 模型定义

线性回归的目标是通过输入变量的线性组合预测连续目标变量，其核心形式为：

$$y(x, w) = w_0 + w_1x_1 + \cdots + w_Dx_D$$

其中：
- $x = (x_1, x_2, \ldots, x_D)^T$ 是 D 维输入向量
- $w = (w_0, w_1, \ldots, w_D)^T$ 是可学习参数
- $w_0$ 称为偏置参数

该模型的关键特性是对参数 $w$ 呈线性，对输入 $x$ 也呈线性，但通过引入基函数可扩展为非线性模型。

### 1.2 基函数扩展

为增强模型表达能力，可将输入映射到非线性基函数的线性组合：

$$y(x, w) = w_0 + \sum_{j=1}^{M-1} w_j\phi_j(x) = \sum_{j=0}^{M-1} w_j\phi_j(x) = w^T\phi(x)$$

其中：
- $\phi(x) = (\phi_0(x), \phi_1(x), \ldots, \phi_{M-1}(x))^T$ 是基函数向量
- $\phi_0(x) = 1$ 对应偏置项

### 1.3 常见基函数类型

- **多项式基函数**：$\phi_j(x) = x^j$，用于捕获输入的非线性关系
- **高斯基函数**：$\phi_j(x) = \exp\left(-\frac{(x-\mu_j)^2}{2s^2}\right)$，局部化输入空间的特征
- **Sigmoid基函数**：$\phi_j(x) = \sigma\left(\frac{x-\mu_j}{s}\right)$，其中 $\sigma(a) = \frac{1}{1+\exp(-a)}$，引入非线性变换

## 二、基函数详解

基函数是线性模型中用于将输入数据映射到更高维特征空间的函数，通过其线性组合来增强模型对复杂关系的表达能力。

### 2.1 基函数的定义与本质

基函数是一组预先定义的非线性函数，记为 $\{\phi_j(x)\}$。在线性模型中，输入变量 $x$ 首先通过基函数映射到新的特征空间，再进行线性组合，形成最终的模型输出。其核心形式为：

$$y(x, w) = \sum_{j=0}^{M-1} w_j \phi_j(x) = w^T \phi(x)$$

其中：
- $w$ 是可学习的权重参数
- $\phi(x) = (\phi_0(x), \phi_1(x), \ldots, \phi_{M-1}(x))^T$ 是基函数向量
- $\phi_0(x) = 1$ 对应偏置项  


### 2.2 基函数的核心作用

#### 2.2.1 引入非线性能力

尽管模型对参数 $w$ 是线性的，但基函数本身的非线性特性使模型能拟合非线性关系。例如，多项式基函数 $\phi_j(x) = x^j$ 可将线性模型扩展为多项式回归，捕捉输入 $x$ 的非线性变化。

#### 2.2.2 特征空间转换

将原始输入映射到更高维空间，使线性模型能处理原始空间中非线性可分的问题。例如，在分类任务中，高斯基函数可将输入映射到径向对称的特征空间，便于线性决策边界划分类别。

#### 2.2.3 灵活调整模型复杂度

通过选择基函数的类型和数量 $M$，可控制模型的表达能力。更多基函数对应更复杂的模型，但需防止过拟合。  


### 2.3 常见基函数类型

#### 2.3.1 多项式基函数

- **形式**：$\phi_j(x) = x^j$（单变量）或 $x_1^{j_1}x_2^{j_2}\cdots x_D^{j_D}$（多变量）
- **特点**：简单直观，适用于捕捉输入的多项式关系，但高次项易导致参数爆炸（如 $D$ 维输入的 $M$ 次多项式有 $O(D^M)$ 个参数）

#### 2.3.2 高斯基函数（径向基函数）

- **形式**：$\phi_j(x) = \exp\left(-\frac{\|x - \mu_j\|^2}{2s^2}\right)$，其中 $\mu_j$ 是中心，$s$ 是宽度
- **特点**：局部化响应，仅在 $x$ 接近 $\mu_j$ 时激活，适用于捕捉局部特征，如图像中的局部模式

#### 2.3.3 Sigmoid基函数

- **形式**：$\phi_j(x) = \sigma\left(\frac{x - \mu_j}{s}\right)$，其中 $\sigma(a) = \frac{1}{1 + \exp(-a)}$
- **特点**：非线性激活，模拟神经元响应，常用于神经网络隐藏层，可堆叠形成多层非线性变换

#### 2.3.4 傅里叶基函数

- **形式**：$\phi_j(x) = \sin(jx)$ 或 $\cos(jx)$
- **特点**：适用于周期性数据（如时间序列），通过频域分解捕捉全局模式

#### 2.3.5 小波基函数

- **形式**：由母小波平移和缩放生成，如哈尔小波、Daubechies小波
- **特点**：同时在时域和频域局部化，适用于信号处理和图像压缩，可捕捉多尺度特征  


### 2.4 基函数与线性模型的关系

#### 2.4.1 线性模型的泛化

原始线性模型（如 $y = w_0 + w_1x$）可视为基函数 $\phi_0(x)=1, \phi_1(x)=x$ 的特例。通过扩展基函数，线性模型可转化为广义线性模型（GLM），如逻辑回归（基函数+Sigmoid激活）。

#### 2.4.2 参数求解与正则化

基函数映射后，模型参数可通过最小二乘法（如正规方程 $w_{ML} = (\Phi^T\Phi)^{-1}\Phi^T t$）或正则化方法（如 $w = (\lambda I + \Phi^T\Phi)^{-1}\Phi^T t$）求解，其中 $\Phi$ 是基函数构成的设计矩阵。  


### 2.5 基函数的局限性与改进

#### 2.5.1 维度灾难

固定基函数（如多项式）在高维输入下参数数量指数增长，导致计算复杂度过高。改进方法：
- 采用数据驱动的基函数（如神经网络自动学习基函数）
- 使用稀疏基函数（如仅选择部分关键基函数激活）

#### 2.5.2 缺乏自适应能力

固定基函数无法根据数据动态调整，可能无法捕捉复杂模式。深度学习通过多层神经网络自动学习基函数（如卷积神经网络的卷积核、Transformer的注意力机制），实现基函数的自适应优化。  


### 2.6 基函数在深度学习中的延伸

在深度学习中，基函数的概念演变为"特征提取器"，但不再是固定函数，而是通过神经网络层自动学习：

- **卷积神经网络（CNN）**：卷积核可视为局部基函数，通过训练自适应调整中心和权重
- **循环神经网络（RNN）**：隐藏层状态转移函数可视为时序基函数，捕捉序列数据的动态特征
- **Transformer**：注意力机制通过加权求和生成动态基函数，适应不同输入的特征映射

基函数的本质是将原始数据转换为更适合建模的特征空间，而深度学习通过端到端训练实现了基函数的自适应学习，突破了传统固定基函数的限制。



## 三、最大似然估计与最小二乘法

### 3.1 似然函数与高斯噪声假设

假设目标变量 $t$ 由确定性函数 $y(x, w)$ 加高斯噪声生成：

$$t = y(x, w) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)$$

则条件概率分布为：

$$p(t|x, w, \sigma^2) = \mathcal{N}(t|w^T\phi(x), \sigma^2)$$

对于独立同分布的数据集 $\{(x_n, t_n)\}_{n=1}^N$，似然函数为：

$$p(t|X, w, \sigma^2) = \prod_{n=1}^N \mathcal{N}(t_n|w^T\phi(x_n), \sigma^2)$$

取对数后得到对数似然函数：

$$\ln p(t|X, w, \sigma^2) = -\frac{N}{2}\ln\sigma^2 - \frac{N}{2}\ln(2\pi) - \frac{1}{\sigma^2}E_D(w)$$

其中，$E_D(w) = \frac{1}{2}\sum_{n=1}^N(t_n - w^T\phi(x_n))^2$ 是平方和误差函数。

### 3.2 参数求解：正规方程

最大化对数似然等价于最小化平方和误差函数。对 $w$ 求导并令梯度为零，可得：

$$\nabla_w E_D(w) = \sum_{n=1}^N (w^T\phi(x_n) - t_n)\phi(x_n) = 0$$

解得最大似然解（正规方程）：

$$w_{ML} = (\Phi^T\Phi)^{-1}\Phi^T t$$

其中：
- $\Phi$ 是 $N \times M$ 设计矩阵，元素为 $\Phi_{nj} = \phi_j(x_n)$
- $t = (t_1, t_2, \ldots, t_N)^T$ 是目标向量

该解对应数据在基函数张成空间的正交投影。

### 3.3 方差估计

噪声方差的最大似然估计为：

$$\sigma_{ML}^2 = \frac{1}{N}\sum_{n=1}^N(t_n - w_{ML}^T\phi(x_n))^2$$

该估计存在偏差，无偏估计需修正为 $\hat{\sigma}^2 = \frac{N}{N-M}\sigma_{ML}^2$，其中 $M$ 是基函数数量。

## 四、正则化与顺序学习

### 4.1 正则化最小二乘法

为防止过拟合，在误差函数中添加正则化项：

$$E(w) = E_D(w) + \frac{\lambda}{2}w^Tw$$

其中，$\lambda$ 是正则化系数，控制数据拟合与参数平滑的权衡。正则化后的参数解为：

$$w = (\lambda I + \Phi^T\Phi)^{-1}\Phi^T t$$

该方法通过约束参数幅值，减少模型复杂度，缓解高维空间中的过拟合问题。

### 4.2 顺序学习（在线学习）

对于大规模数据，采用随机梯度下降迭代更新参数：

$$w^{(\tau+1)} = w^{(\tau)} + \eta(t_n - {w^{(\tau)}}^T\phi(x_n))\phi(x_n)$$

其中，$\eta$ 是学习率，每次使用单个数据点更新参数，适用于流式数据和内存受限场景。

## 五、几何解释与多输出扩展

### 5.1 最小二乘的几何意义

误差函数 $E_D(w)$ 等价于目标向量 $t$ 与预测向量 $y = \Phi w$ 的欧氏距离平方。最优解 $w_{ML}$ 对应 $t$ 在基函数空间的正交投影，即 $y = \Phi w_{ML}$ 是 $t$ 在该空间的最佳近似。

### 5.2 多输出线性回归

对于K维目标向量 $t = (t_1, t_2, \ldots, t_K)^T$，模型扩展为：

$$y(x, W) = W^T\phi(x)$$

其中，$W$ 是 $M \times K$ 参数矩阵。最大似然解为：

$$W_{ML} = (\Phi^T\Phi)^{-1}\Phi^T T$$

其中，$T$ 是 $N \times K$ 目标矩阵，每列对应一个输出变量。

## 六、关键结论与注意事项

### 6.1 线性回归的局限性

模型对基函数的选择敏感，固定基函数可能无法捕获复杂数据模式，需结合领域知识或数据驱动方法选择基函数。

### 6.2 正则化的作用

$\lambda$ 控制模型复杂度，过小导致过拟合，过大导致欠拟合，需通过交叉验证优化。

### 6.3 计算复杂度

正规方程求解需计算 $M \times M$ 矩阵的逆，当 $M$ 较大时计算成本高，可采用迭代优化或稀疏正则化降低复杂度。