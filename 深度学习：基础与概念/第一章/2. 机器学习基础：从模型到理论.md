# 机器学习基础：从模型到理论

## 合成数据

通过构建一个从正弦函数采样生成的合成数据集，引入了机器学习中的一些基本概念

现实世界数据集的特点，即存在基本规律（如正弦函数），但每个观测值都受随机噪声干扰

在机器学习中，准确预测以前未见过的输入是一个关键目标，被称为泛化能力。
在机器学习的实际应用中，我们的目标是在有限的训练集中发现数据的潜在趋势。了解生成数据的过程可以帮助我们阐明机器学习中的重要概念

## 线性模型
模型的定义与目标：旨在利用训练集预测新输入变量 $x_0$ 对应的目标变量t的值。

为了实现这个目标，采用了基于曲线拟合的简单方法，使用多项式函数 $y(x,w)=w_0 + w_1x + w_2x^2 +... + w_Mx^M=\sum_{j = 0}^{M}w_jx^j$ 来拟合数据。
虽然该多项式函数对x是非线性的，但对系数w是线性的。这种在未知参数中呈线性的函数在机器学习中被称为线性模型，它具有一些重要的性质，但同时也存在显著的限制 

**模型的局限性与应对方法**

从数据中学习潜在函数 $\sin(2 \pi x)$ 是一个具有挑战性的问题。一方面，需要从有限的数据集推广到整个函数，这本身就具有难度；另一方面，观察到的数据受到噪声的干扰，这使得对于给定的变量 x，变量 t 的适当值存在不确定性。在实际应用中，为了应对这种不确定性，概率论提供了一个精确和定量表达不确定性的框架。决策理论则允许我们利用这种概率表示进行预测，这些预测根据适当的标准是最优的

从数据中学习概率是机器学习的核心内容

## 误差函数

**误差函数的定义**
为了确定多项式系数w的值，需要将多项式拟合到训练数据，而这一过程是通过最小化误差函数来实现的

在众多可选的误差函数中，一种常用的选择是计算每个数据点的预测值 $y(x_n,w)$ 与目标值 $t_n$ 差的平方和的一半，即 $E(w)=\frac{1}{2}\sum_{n = 1}^{N}\{y(x_n,w) - t_n\}^2$

这个误差函数具有非负的特性，当且仅当函数 $y(x,w)$ 通过每个训练数据点时，其值为零

**误差函数的求解与作用**

由于误差函数是关于w的二次函数，对其求导后得到的导数是 w 元素的线性函数

基于这个特性，可以通过闭式求解找到误差函数的最小值 $w^*$。得到 $w^*$ 后，就能确定拟合多项式 $y(x,w^*)$

在后续从概率论的角度推导以及理解整个模型的优化过程中，误差函数都起着关键作用，为评估模型的拟合效果提供了一个量化的标准，通过不断调整 w 来最小化误差函数，使得模型的预测值尽可能接近真实值，从而实现模型的优化

## 模型复杂度
- 模型复杂度指的是模型拟合数据和捕捉潜在模式的能力强弱，通常与模型中可调整参数的数量、函数形式的灵活性等相关
- 复杂模型能拟合更复杂的模式，但也更易过拟合；简单模型灵活性低，可能欠拟合

拟合曲线波动剧烈，并且对函数的表示非常差。 这种行为被称为 **过拟合** 
具有较大 M 值的更灵活的多项式会越来越适应目标值上的随机噪声
通过更大的数据集，可以承担对数据拟合更复杂（更灵活）的模型

**模型复杂度的定义与本质**
- 核心内涵：模型复杂度衡量模型对数据的拟合能力及函数形式的 “灵活性”。
例如，多项式回归中，多项式的阶数 M 越高（如 M=9 比 M=3），模型可调整的参数越多，能拟合的曲线形状越复杂，复杂度越高。
- 与参数数量的关系：一般来说，模型中可学习参数（如权重、偏置）的数量越多，复杂度越高。但需注意，参数数量并非唯一标准，函数形式的非线性程度等也会影响复杂度。

**模型复杂度与泛化能力的矛盾：偏差 - 方差权衡**
- 偏差：模型预测值与真实值的平均差异，低复杂度模型因灵活性不足，偏差较大（如 M=0 时无法拟合正弦曲线）。
- 方差：模型在不同训练集上预测结果的波动程度，高复杂度模型因过度拟合单个训练集的噪声，方差较大（如 M=9 时不同数据集的拟合曲线差异大）。
权衡关系：随着模型复杂度增加，偏差逐渐减小，方差逐渐增大。最优模型复杂度需在偏差和方差间找到平衡，使测试集误差最小。

**模型复杂度的控制方法**
1. 正则化：在误差函数中添加惩罚项（如权重衰减），限制高复杂度模型的参数幅度，降低方差。例如，对 M=9 的多项式添加正则化项后，系数幅度减小，振荡减弱，测试误差降低。
2. 数据集大小：增大训练集规模可允许使用更高复杂度的模型。例如，M=9 的多项式在 N=100 个数据点时，过拟合问题减轻。
3. 超参数选择：通过交叉验证等方法选择最优复杂度超参数（如多项式阶数 M、正则化系数 λ），避免凭经验选择导致的过拟合或欠拟合。

> 模型复杂度的核心问题：
>
>复杂度需与数据规模、任务难度匹配，盲目追求高复杂度会导致过拟合，而忽视复杂度则无法捕捉真实模式


## 正则化
正则化在机器学习中的应用，旨在解决模型过拟合问题，提升模型泛化能力

**基本原理**

正则化通过向误差函数添加惩罚项，避免系数取值过大。以简单的惩罚项为例，其形式为所有系数的平方和，使得修改后的误差函数为
$$\tilde{E}(w)=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,w) - t_n\}^2 + \frac{\lambda}{2}\|w\|^2$$ 
其中 $\|w\|^2 = w_0^2 + w_1^2 +... + w_M^2$，$\lambda$ 用于调节正则化项与平方误差项的相对重要性。

通常情况下，正则化项中的系数 $w_0$ 会被省略，因为其存在可能导致结果依赖于目标变量的选择起点；当然，也可将其包含在内，但需为其设置单独的正则化系数。在神经网络领域，这种方法被称作权重衰减，它能够促使权重衰减至零。

应用示例：以将阶数为 $M = 9$ 的多项式拟合到数据集为例，使用正则化误差函数进行拟合。结果显示，当 $\ln \lambda = -18$ 时，过拟合现象得到有效抑制，拟合曲线更接近真实函数 $\sin(2\pi x)$；然而，当 $\ln \lambda = 0$ 时，由于 $\lambda$ 值过大，拟合效果反而不佳。这表明，$\lambda$ 能够控制模型的有效复杂度，进而决定过拟合的程度。

**超参数选择**

像 $\lambda$ 这样在模型训练时用于控制模型复杂度，但不能通过直接优化误差函数来确定的参数，被称为超参数。在确定模型参数w时，不能简单地同时对误差函数和超参数进行最小化，否则会导致 $\lambda \to 0$ ，进而产生过拟合模型。确定超参数合适值的常用方法是将数据划分为训练集和验证集，选择在验证集上误差最小的模型。当数据量有限时，为充分利用数据进行训练，可采用交叉验证技术，如将数据划分为S个相等大小的组，进行S次训练和评估，最后对性能得分取平均值。不过，交叉验证的缺点是会增加训练次数，对于计算成本较高的模型而言，可能会带来一定问题

## 模型选择

如何在机器学习中确定最优模型复杂度（如多项式阶数、正则化系数等超参数），避免因盲目选择导致过拟合或欠拟合

### 模型选择的核心问题

- **超参数的定义**：超参数是控制模型复杂度但不通过训练直接学习的参数，例如多项式拟合中的阶数M、正则化系数λ、神经网络中的隐藏层数量等。
- **关键挑战**：若仅通过优化训练集误差确定超参数（如选M使训练误差最小），会导致模型复杂度过高（如M=9），陷入过拟合；反之，若忽略数据特性选简单模型，可能欠拟合。

### 模型选择的常用方法

####  **验证集与测试集划分**
- **基本思路**：将数据分为三部分：
    - **训练集**：用于拟合模型参数（如多项式系数w）。
    - **验证集（开发集）**：用于选择超参数（如M、λ），选择在验证集上误差最小的模型。
    - **测试集**：用于最终评估模型泛化能力，避免对验证集过拟合。
- **示例**：在多项式拟合中，通过验证集比较不同M对应的误差，选使验证误差最小的 M

####  **交叉验证（Cross-Validation）**
- **适用场景**：当数据量有限时，划分验证集会导致训练数据减少，此时用交叉验证重复利用数据。
- **实现方式**：
    - **K折交叉验证**：将数据分为K个相等子集，每次用K-1个子集训练，1个子集验证，重复K次取平均误差。
    - **留一法（Leave-One-Out）**：K=N（N为数据点总数），每次留1个点验证，适用于极小数据集。
- **优缺点**：
    - 优点：充分利用数据，评估更可靠。
    - 缺点：计算成本高（训练次数×K），对大规模数据或复杂模型不友好。

####  **贝叶斯模型选择（隐含在思想中）**
- **核心思想**：通过先验分布对模型复杂度惩罚，如用贝叶斯定理计算后验概率 $p(M|D) \propto p(D|M)p(M)$，其中 $p(M)$ 对复杂模型（大M）赋予低先验概率。
- **与正则化的联系**：正则化可视为贝叶斯模型选择的近似，如权重衰减对应高斯先验。


### 实际应用中的挑战与策略
####  **超参数搜索的复杂度**
- **问题**：当超参数（如λ、M）较多时，穷举搜索（如网格搜索）计算量呈指数增长（如2个超参数各10个值需100次训练）。
- **优化策略**：
    - **随机搜索**：随机采样超参数组合，效率高于网格搜索（Bergstra & Bengio, 2012）。
    - **贝叶斯优化**：用高斯过程建模超参数-误差关系，迭代推荐最优值。

####  **深度学习中的模型选择**
- **特点**：深度模型超参数（如学习率、层数、批量大小）更多，且训练成本高。
- **常用方法**：
    - **早停（Early Stopping）**：训练时监控验证误差，当误差不再下降时停止，避免过拟合。
    - **学习率调度**：动态调整学习率，间接控制模型复杂度。
    - **迁移学习**：用预训练模型的超参数作为初始化，减少搜索空间。

####  **模型复杂度与计算资源的平衡**
- **大模型与大数据**：深度学习中，大规模数据集（如ImageNet）可支撑高复杂度模型（如ResNet），此时过拟合风险降低，模型容量（参数数量）与数据量正相关。
- **小数据场景**：用正则化（如Dropout）、数据增强（如旋转图像）增加有效数据量，避免小数据下的过拟合。

### 模型选择的本质：归纳偏差的显式控制
- **归纳偏差**：模型对特定解的偏好（如正则化偏好简单解）。
- **模型选择的本质**：通过超参数显式设定归纳偏差，例如：
    - 选小M或大λ：偏好简单模型，减少方差（抗过拟合）。
    - 选大M或小λ：偏好复杂模型，减少偏差（拟合能力强）。
- **深度学习的启示**：深度模型的架构设计（如CNN的卷积层）本身就是归纳偏差（如平移等变性），结合超参数选择，可平衡模型复杂度与泛化能力。

> 模型选择的核心逻辑：**超参数优化是在偏差与方差间寻找平衡点，需通过验证集或交叉验证量化评估，而非仅依赖训练集表现**
