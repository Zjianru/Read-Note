# 机器学习简史：从感知机到深度学习

## （一）单层网络与感知机局限
单层网络是人工神经网络的基础形态，其设计灵感源于对生物神经元的简化建模。典型代表是感知机（Perceptron），由 Rosenblatt 于 1962 年提出，其数学模型可表示为：

$$f(x) = \begin{cases} 
1, & \text{if } w^T x + b > 0 \\
0, & \text{otherwise}
\end{cases}$$

其中，$x = (x_1, x_2, \dots, x_D)^T$ 是输入向量，$w = (w_1, w_2, \dots, w_D)^T$ 是权重向量，b 是偏置项。感知机的核心是通过线性组合输入特征并施加阶跃激活函数，实现二分类决策，其几何意义是在输入空间中定义一个超平面作为决策边界。

感知机采用增量式训练算法（感知机学习规则），步骤如下：初始化：权重 w 和偏置 b 初始化为小随机值。迭代更新：对每个训练样本 $(x_n, t_n)$（$t_n \in \{0, 1\}$ 为标签），计算预测值 $f(x_n)$。若预测错误（即 $f(x_n) \neq t_n$），则按以下规则更新参数：

$$w \leftarrow w + \eta(t_n - f(x_n))x_n, \quad b \leftarrow b + \eta(t_n - f(x_n))$$

其中 $\eta > 0$ 是学习率。收敛条件：若存在一组权重能完美分类所有训练数据，算法会在有限步内收敛（Rosenblatt 收敛定理）。

**单层网络的关键限制**

线性可分性瓶颈Minsky 和 Papert 在 1969 年的著作《Perceptrons》中指出，单层网络存在根本性缺陷：无法解决线性不可分问题。典型反例是异或（XOR）问题：输入 $(0,0)$ 和 $(1,1)$ 对应标签 0，$(0,1)$ 和 $(1,0)$ 对应标签 1。由于 XOR 的决策边界是非线性的（如抛物线），单层感知机无法用线性超平面正确分类所有样本

> 尽管感知器在实际机器学习中早已消失，但这个名字仍然存在，因为现代神经网络有时也被称为多层感知器或MLP

## （二）反向传播：多层网络训练的核心算法

反向传播（Backpropagation）是用于训练多层神经网络的关键算法，其核心基于微积分中的链式法则，通过计算误差在网络中的反向传播梯度，来更新网络各层的参数（权重和偏置），以最小化预测结果与真实标签间的误差。

当神经网络处理数据时，先进行"前向传播"，把输入数据通过各层神经元的计算，得到输出结果；接着"反向传播"，从输出层开始，依据预测误差，从后往前逐层计算参数对误差的影响（梯度），再利用优化算法（如梯度下降）调整参数，让网络下次预测更精准。

### 算法关键步骤

**1. 前向传播（Forward Propagation）**

从输入层开始，数据依次经过隐藏层（若有多层隐藏层则逐层传递），最终到输出层。每层神经元先对输入进行线性变换：

$$z = Wx + b$$

其中 $W$ 是权重矩阵，$x$ 是输入向量，$b$ 是偏置向量。然后通过非线性激活函数（如 Sigmoid、ReLU 等）引入非线性，让网络能拟合复杂关系，得到该层输出，传递给下一层，直至输出层得出预测值。

**2. 计算误差（Error Calculation）**

在输出层，对比预测值和真实标签，用误差函数算出误差，衡量预测结果与真实值的差距。常用的误差函数包括：

- **均方误差（MSE）**：$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_{pred_i} - y_{true_i})^2$$
- **交叉熵损失**：常用于分类任务

**3. 反向传播梯度（Backward Propagation of Gradients）**

依据链式法则，从输出层往回，逐层计算误差对各层参数（权重 $W$、偏置 $b$）的偏导数（梯度）。具体过程：

- 计算输出层权重的梯度：先算误差对输出层激活前结果（$z$）的梯度
- 结合激活函数导数、前一层输出，得到权重梯度
- 隐藏层参数梯度计算：结合后一层传来的梯度、当前层激活函数导数和输入等

**4. 参数更新（Parameter Update）**

用优化算法根据反向传播得到的梯度，调整各层参数。以随机梯度下降（SGD）为例：

$$W = W - \eta\nabla W$$

其中 $\eta$ 是学习率，$\nabla W$ 是权重梯度。通过不断调整参数，使误差函数值减小，让网络预测更准确。

### 名词解释

**前馈神经网络（Feedforward Neural Network，FNN）**

一种基础的人工神经网络结构，核心特点是信息单向流动，从输入层经隐藏层到输出层，无循环或反馈连接。

#### 一、基本结构

由输入层、隐藏层（可多层）、输出层组成，层间神经元全连接（或局部连接，如卷积神经网络变体）：

- **输入层**：接收原始数据（如图像像素、文本特征），神经元数量与输入特征维度匹配（如 28×28 像素图像对应 784 个输入神经元）
- **隐藏层**：提取数据特征，通过"加权求和 + 非线性激活"处理输入；层数、神经元数按需调整，引入非线性激活函数（如 ReLU、Sigmoid），让网络拟合复杂关系
- **输出层**：输出预测结果，神经元数量匹配任务（分类任务用 Softmax 输出概率，回归任务直接输出数值）

#### 二、工作流程

**前向传播**

输入数据从输入层→隐藏层→输出层单向传递，每层神经元先对输入做线性变换：

$$z = Wx + b$$

其中 $W$ 为权重矩阵，$b$ 为偏置向量，再经激活函数非线性变换，最终输出层生成预测。

**训练优化**

结合反向传播算法，先通过前向传播得预测，计算与真实标签的误差（如均方误差、交叉熵），再反向逐层求误差对参数（$W$、$b$）的梯度，用优化器（如梯度下降）更新参数，最小化误差。

#### 三、关键特性

**单向无反馈**

信息流严格"输入→输出"，无循环连接，计算高效、易并行实现，是"万能逼近定理"的结构基础（足够层数和神经元可逼近任意复杂函数）。

**静态处理**

每次处理独立，无记忆性，专注当前输入；但也导致难直接处理序列依赖（如语言、时间序列），催生 RNN、Transformer 等变体。

## （三）深度网络：从多层到深度学习的突破

一系列技术创新让多层权重神经网络可被有效训练，突破了"深度"带来的训练瓶颈，深度神经网络（Deep Neural Networks，DNNs）由此成为研究和应用焦点，专注这类网络的机器学习子领域就叫深度学习（Deep Learning），它让神经网络能挖掘更复杂数据模式。

多层结构让深度神经网络可逐层提取数据特征：

- **图像处理**：浅层可能提取边缘、纹理等基础特征，深层组合这些基础特征，识别更复杂的物体、场景
- **文本处理**：浅层抓字词基本语义，深层构建语义关联、情感倾向等

相比传统浅层网络，对复杂任务适配性、效果提升显著。

### 推动深度网络发展的关键因素
#### （一）表示学习（Representation Learning）

**概念**

深度神经网络里，隐藏层负责"提炼"数据特征，表示学习就是让网络自动把原始数据（如图像像素、文本字词）转化成更有语义、更简洁且利于后续任务的新表示形式。比如处理猫咪图片，原始是像素点，经表示学习，网络能提取出"毛发纹理""轮廓形状"等高层特征。

**价值**

- **降低处理难度**：高层特征更聚焦关键信息，降低后续网络层处理难度
- **特征复用**：这些"学"到的内部表示，能通过迁移学习（把在 A 任务上学到的特征、参数，用到 B 任务）复用，像图像识别模型学的特征，稍作调整可用于图像分类、目标检测等不同任务
#### （二）残差连接（Residual Connection）

传统深度网络训练时，信号从输入层经多层传递到输出层，反向传播回传梯度时，容易出现"梯度消失"（梯度越传越小，深层网络参数更新不动）。

残差连接（把网络某层输入直接加到后续层输出），就像给梯度回传搭了"高速通道"，让梯度更顺畅传递，解决深层网络训练难题，助力上百层、上千层的极深网络（如 ResNet 系列）落地应用。
#### （三）自动微分（Automatic Differentiation）

训练深度网络要算"误差函数对参数的梯度"（反向传播核心），自动微分技术能基于前向传播的计算逻辑，自动生成求梯度的代码。

以前得人工推导复杂梯度公式，现在研究人员只需写简洁前向传播代码，就能快速探索不同网络结构、尝试参数更新，大大提升研发效率，让复杂深度网络设计、训练更可行。
#### （四）开源与协同创新模式

机器学习领域很多研究依托开源模式，比如公开模型架构、训练代码、数据集等。这就像大家共享"知识积木"，研究人员能站在前人成果上，快速构建、拓展新模型，加速整个深度网络领域创新，让更多人（包括初学者）能接触、参与到前沿研究与应用实践里。
#### （五）基础模型（Foundation Model）

像大语言模型（如 GPT 系列）就是典型，它们用海量多样数据训练，能学习到通用知识、语义模式，然后通过"微调"（在特定小数据集上适配具体任务），解决翻译、问答、文本生成等不同下游任务。

这种模式让模型具备广泛适用性，减少重复造轮子，推动深度网络在多领域落地。