# 线性模型

## 基本形式

给定由 d 个属性描述的示例 $x = (x_1, x_2, ..., x_d)$，其中每个属性 $x_i$ 都是数值型的，
线性模型 `linear model` 试图学得一个通过属性的线性组合来进行预测的函数，预测一个实值输出 $y$。

线性模型的形式如下：

$$
y = w_1x_1 + w_2x_2 + ... + w_dx_d + b
$$

其中，$w_1, w_2, ..., w_d$ 是权重（weight），$b$ 是偏置（bias），它们是需要学习的参数。

一般用向量写成：

$$
y = w^Tx + b
$$

其中，$w = (w_1, w_2, ..., w_d)$ 是权重向量，$b$ 是偏置，$x = (x_1, x_2, ..., x_d)$ 是输入向量，$w^T$ 表示权重向量的转置，$w^Tx$ 表示权重向量和输入向量的内积。

线性模型形式简单、易于建模，但却蕴涵着机器学习中一些重要的基本思想。许多功能更为强大的非线性模型 `nonlinear model` 可在线性模型的基础上通过引入层级结构或高维映射而得。此外，由于 w 直观表达了各属性在预测中的重要性，因此线性模型有很好的 **可解释性** `comprehensibility`
> 亦称 **可理解性** `understandability`

## 线性回归
给定数据集 D={(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}，其中每个样本由输入向量 $x_i = (x_{i1}, x_{i2}, ..., x_{id})$ 和输出值 $y_i$ 组成。

线性回归 `linear regression` 试图学得一个线性模型，以尽可能准确地预测实值输出标记，使得模型预测值与真实值之间的误差最小。

线性回归的损失函数 `loss function` 通常是均方误差 `mean squared error`，即：

$$
L(w, b) = \frac{1}{N} \sum_{i=1}^N (y_i - (w^Tx_i + b))^2
$$

其中，$N$ 是样本数，$y_i$ 是第 $i$ 个样本的输出值，$w^Tx_i + b$ 是第 $i$ 个样本的预测值。

均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称**欧氏距离**`Euclidean distance`。

基于均方误差最小化来进行模型求解的方法称为“最小二乘法”`least square method`。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

求解 w 和 b 使最小化的过程，称为线性回归模型的最小二乘“参数估计”`parameter estimation`

更一般的情形是如本节开头的数据集D，样本由 d 个属性描述。此时我们试图学得

$$
f(x_i) = w^T x_i + b
$$

使得 $f(x_i) \approx y_i$

这称为 **多元线性回归** `multivariate linear regression`

> 亦称**多变量线性回归**


“## 广义线性模型

线性模型虽简单，却有丰富的变化。例如对于样例 $(x, y)$，$y \in \mathbb{R}$，当我们希望线性模型的预测值逼近真实标记 $y$ 时，就得到了线性回归模型。为便于观察，我们把线性回归模型简写为：

$$
y = w^T x + b
$$

可否令模型预测值逼近 $y$ 的衍生物呢？譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即：

$$
\ln y = w^T x + b
$$

这就是"对数线性回归"（log-linear regression），它实际上是在试图让 $e^{w^T x + b}$ 逼近 $y$。该式在形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射，如图3.1所示。这里的对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用。

更一般地，考虑单调可微函数 $g(\cdot)$，令：

$$
y = g^{-1}(w^T x + b) 
$$

这样得到的模型称为**广义线性模型**`generalized linear model`，其中函数 $g(\cdot)$ 称为**联系函数**`link function`。

显然，对数线性回归是广义线性模型在 $g(\cdot) = \ln(\cdot)$ 时的特例。
